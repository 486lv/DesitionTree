{'纹理': {'模糊': 'ng', '清晰': {'根蒂': {'硬挺': 'ng', '稍蜷': {'色泽': {'乌黑': {'触感': {'硬滑': 'ok', '软粘': 'ng'}}, '青绿': 'ok'}}, '蜷缩': 'ok'}}, '稍糊': {'触感': {'硬滑': 'ng', '软粘': 'ok'}}}}



[['硬滑' '青绿' '浊响' '清晰' '凹陷' '蜷缩' 'ok']
 ['硬滑' '乌黑' '沉闷' '清晰' '凹陷' '蜷缩' 'ok']
 ['硬滑' '乌黑' '浊响' '清晰' '凹陷' '蜷缩' 'ok']
 ['硬滑' '青绿' '沉闷' '清晰' '凹陷' '蜷缩' 'ok']
 ['硬滑' '浅白' '浊响' '清晰' '凹陷' '蜷缩' 'ok']
 ['软粘' '青绿' '浊响' '清晰' '稍凹' '稍蜷' 'ok']
 ['硬滑' '乌黑' '浊响' '清晰' '稍凹' '稍蜷' 'ok']
 ['软粘' '青绿' '清脆' '清晰' '平坦' '硬挺' 'ng']
 ['软粘' '乌黑' '浊响' '清晰' '稍凹' '稍蜷' 'ng']

3 1  2-》 

  6  6 -》0



1   0

3 1 2

5 0

[['硬滑' '青绿' '清脆' '模糊' '凹陷' '稍蜷']
 ['软粘' '乌黑' '浊响' '稍糊' '平坦' '硬挺']
 ['软粘' '青绿' '浊响' '模糊' '凹陷' '蜷缩']
 ['硬滑' '乌黑' '清脆' '稍糊' '稍凹' '硬挺']
 ['软粘' '乌黑' '清脆' '模糊' '平坦' '硬挺']
 ['软粘' '青绿' '清脆' '模糊' '稍凹' '蜷缩']
 ['硬滑' '浅白' '清脆' '清晰' '凹陷' '蜷缩']
 ['软粘' '乌黑' '清脆' '模糊' '凹陷' '蜷缩']
 ['软粘' '青绿' '清脆' '稍糊' '凹陷' '硬挺']
 ['硬滑' '乌黑' '清脆' '模糊' '平坦' '稍蜷']
 ['软粘' '青绿' '浊响' '清晰' '平坦' '稍蜷']
 ['软粘' '浅白' '清脆' '清晰' '平坦' '硬挺']
 ['硬滑' '青绿' '浊响' '模糊' '凹陷' '稍蜷']
 ['硬滑' '青绿' '清脆' '稍糊' '凹陷' '硬挺']
 ['硬滑' '浅白' '浊响' '清晰' '凹陷' '稍蜷']
 ['硬滑' '乌黑' '沉闷' '稍糊' '凹陷' '稍蜷']
 ['软粘' '青绿' '清脆' '模糊' '凹陷' '硬挺']
 ['软粘' '乌黑' '沉闷' '清晰' '稍凹' '蜷缩']
 ['硬滑' '乌黑' '沉闷' '稍糊' '凹陷' '稍蜷']
 ['软粘' '浅白' '清脆' '稍糊' '平坦' '硬挺']
 ['硬滑' '青绿' '浊响' '清晰' '平坦' '蜷缩']
 ['软粘' '浅白' '浊响' '稍糊' '凹陷' '蜷缩']
 ['硬滑' '乌黑' '清脆' '稍糊' '稍凹' '稍蜷']
 ['软粘' '青绿' '浊响' '清晰' '稍凹' '硬挺']
 ['软粘' '青绿' '清脆' '模糊' '平坦' '稍蜷']
 ['硬滑' '浅白' '清脆' '清晰' '平坦' '稍蜷']
 ['硬滑' '青绿' '沉闷' '清晰' '平坦' '稍蜷']
 ['硬滑' '浅白' '沉闷' '模糊' '稍凹' '蜷缩']
 ['软粘' '浅白' '沉闷' '清晰' '稍凹' '蜷缩']
 ['硬滑' '乌黑' '浊响' '稍糊' '平坦' '蜷缩']
 ['软粘' '浅白' '沉闷' '清晰' '平坦' '蜷缩']
 ['软粘' '乌黑' '沉闷' '稍糊' '凹陷' '稍蜷']
 ['软粘' '青绿' '清脆' '清晰' '凹陷' '硬挺']
 ['硬滑' '浅白' '清脆' '模糊' '凹陷' '蜷缩']
 ['硬滑' '青绿' '浊响' '清晰' '稍凹' '蜷缩']
 ['硬滑' '浅白' '清脆' '清晰' '稍凹' '硬挺']
 ['硬滑' '乌黑' '沉闷' '清晰' '凹陷' '硬挺']
 ['硬滑' '青绿' '清脆' '模糊' '平坦' '蜷缩']
 ['软粘' '浅白' '浊响' '清晰' '稍凹' '硬挺']
 ['软粘' '青绿' '浊响' '清晰' '稍凹' '稍蜷']
 ['硬滑' '浅白' '沉闷' '清晰' '稍凹' '稍蜷']
 ['硬滑' '乌黑' '沉闷' '清晰' '平坦' '稍蜷']
 ['软粘' '青绿' '沉闷' '模糊' '稍凹' '蜷缩']
 ['软粘' '青绿' '清脆' '稍糊' '稍凹' '硬挺']
 ['软粘' '青绿' '浊响' '模糊' '稍凹' '稍蜷']
 ['软粘' '乌黑' '清脆' '模糊' '凹陷' '稍蜷']
 ['硬滑' '乌黑' '沉闷' '稍糊' '平坦' '稍蜷']
 ['硬滑' '青绿' '浊响' '稍糊' '稍凹' '稍蜷']
 ['硬滑' '青绿' '浊响' '模糊' '凹陷' '硬挺']
 ['硬滑' '青绿' '浊响' '稍糊' '稍凹' '硬挺']
 ['软粘' '浅白' '浊响' '稍糊' '平坦' '稍蜷']
 ['硬滑' '乌黑' '浊响' '稍糊' '稍凹' '稍蜷']
 ['软粘' '浅白' '沉闷' '清晰' '平坦' '蜷缩']
 ['硬滑' '浅白' '浊响' '清晰' '稍凹' '稍蜷']
 ['硬滑' '青绿' '沉闷' '模糊' '凹陷' '稍蜷']
 ['软粘' '浅白' '清脆' '稍糊' '平坦' '硬挺']
 ['硬滑' '青绿' '浊响' '清晰' '凹陷' '硬挺']
 ['硬滑' '浅白' '清脆' '稍糊' '稍凹' '蜷缩']
 ['软粘' '青绿' '浊响' '清晰' '平坦' '硬挺']
 ['软粘' '浅白' '沉闷' '模糊' '凹陷' '硬挺']
 ['硬滑' '青绿' '浊响' '模糊' '稍凹' '硬挺']
 ['软粘' '青绿' '浊响' '稍糊' '稍凹' '蜷缩']
 ['硬滑' '青绿' '沉闷' '模糊' '平坦' '稍蜷']
 ['硬滑' '乌黑' '浊响' '模糊' '凹陷' '硬挺']
 ['软粘' '青绿' '清脆' '稍糊' '平坦' '硬挺']
 ['软粘' '乌黑' '浊响' '模糊' '平坦' '硬挺']
 ['硬滑' '青绿' '浊响' '模糊' '平坦' '硬挺']
 ['硬滑' '青绿' '浊响' '模糊' '凹陷' '蜷缩']
 ['软粘' '浅白' '浊响' '模糊' '平坦' '稍蜷']
 ['软粘' '青绿' '清脆' '清晰' '平坦' '蜷缩']
 ['硬滑' '浅白' '清脆' '模糊' '平坦' '稍蜷']
 ['硬滑' '乌黑' '沉闷' '模糊' '凹陷' '稍蜷']
 ['软粘' '浅白' '浊响' '清晰' '平坦' '稍蜷']
 ['硬滑' '青绿' '浊响' '模糊' '稍凹' '硬挺']
 ['硬滑' '青绿' '浊响' '稍糊' '凹陷' '硬挺']
 ['软粘' '乌黑' '清脆' '清晰' '平坦' '稍蜷']
 ['软粘' '青绿' '沉闷' '模糊' '稍凹' '蜷缩']
 ['硬滑' '浅白' '浊响' '模糊' '平坦' '蜷缩']
 ['硬滑' '浅白' '浊响' '清晰' '凹陷' '硬挺']
 ['软粘' '乌黑' '浊响' '稍糊' '凹陷' '蜷缩']
 ['硬滑' '浅白' '清脆' '稍糊' '稍凹' '硬挺']
 ['软粘' '浅白' '浊响' '模糊' '平坦' '稍蜷']
 ['软粘' '青绿' '沉闷' '稍糊' '平坦' '稍蜷']
 ['软粘' '浅白' '沉闷' '稍糊' '凹陷' '蜷缩']
 ['软粘' '青绿' '清脆' '清晰' '平坦' '硬挺']
 ['软粘' '浅白' '浊响' '清晰' '稍凹' '硬挺']
 ['硬滑' '乌黑' '沉闷' '清晰' '平坦' '稍蜷']
 ['软粘' '浅白' '沉闷' '稍糊' '凹陷' '硬挺']
 ['硬滑' '青绿' '沉闷' '清晰' '凹陷' '硬挺']
 ['硬滑' '青绿' '沉闷' '模糊' '凹陷' '硬挺']
 ['软粘' '乌黑' '沉闷' '稍糊' '平坦' '稍蜷']
 ['硬滑' '青绿' '沉闷' '模糊' '稍凹' '硬挺']
 ['软粘' '青绿' '清脆' '稍糊' '凹陷' '蜷缩']
 ['软粘' '乌黑' '清脆' '模糊' '凹陷' '蜷缩']
 ['软粘' '青绿' '沉闷' '稍糊' '凹陷' '蜷缩']
 ['软粘' '乌黑' '浊响' '清晰' '稍凹' '稍蜷']
 ['软粘' '乌黑' '清脆' '清晰' '平坦' '蜷缩']
 ['硬滑' '青绿' '浊响' '清晰' '稍凹' '稍蜷']
 ['软粘' '浅白' '浊响' '模糊' '凹陷' '硬挺']
 ['软粘' '乌黑' '清脆' '清晰' '凹陷' '稍蜷']]

```
# -*- coding: utf-8 -*-
import matplotlib
import numpy as np
import pydotplus
from graphviz import Digraph
import matplotlib.pyplot as plt
import networkx as nx

def cal_entropy(data, x_labels):
    finish_columns = np.unique(data[:, -1])
    if len(finish_columns) == 1:
        return finish_columns[0]  # 返回ng与ok，表面这个分支已经分到叶子了
    dict_items = dict()  # 创建一个字典,储存有哪些类别
    len_num, len_label = data.shape  # 总类别数, 每一组总的个数
    len_label -= 1
    result = data[:, len_label]
    unique_decision = np.unique(result)  # ok, ng
    len_decision_ok = np.sum(result == unique_decision[0])  # ok的个数
    len_decision_nor = len(result) - len_decision_ok
    old_entropy = -float(len_decision_ok / len_num) * np.log2(float(len_decision_ok / len_num)) - float(len_decision_nor / len_num) * np.log2(float(len_decision_nor / len_num))

    message_entropy = []
    for i in range(len_label):
        item = data[:, i]
        unique_item = np.unique(item)  # 第i种类别的有哪些
        dict_items[x_labels[i]] = unique_item
        len_item = len(unique_item)  # 第i种类别的有多少种
        entropy_item = []
        entropy_item_len = []

        for j in range(len_item):
            # 遍历item，找到unique_item中每一个类别的下标
            indexs = np.where(item == unique_item[j])
            len_indexs = len(indexs[0])  # where返回的是一个元组，第一个元素是下标
            entropy_item_len.append(len_indexs)

            # 遍历result，找到对应的值
            result_item = result[indexs[0]]  # 使用 indexs[0]
            len_ok = np.sum(result_item == unique_decision[0])
            len_nor = len_indexs - len_ok

            # 计算信息熵
            if len_indexs > 0:  # 确保 len_indexs 不为零
                p1 = float(len_ok) / len_indexs
                p2 = float(len_nor) / len_indexs

                # 避免计算 log(0)
                entropy_value = 0
                if p1 > 0:
                    entropy_value -= p1 * np.log2(p1)
                if p2 > 0:
                    entropy_value -= p2 * np.log2(p2)

                entropy_item.append(entropy_value)
            else:
                entropy_item.append(0)

        # 计算加权熵
        entropy = 0
        for j in range(len_item):
            p = float(entropy_item_len[j]) / float(len_num)
            entropy += p * entropy_item[j]
        message_entropy.append(entropy)
    D = []
    print(dict_items)
    for i in message_entropy:
        D.append(old_entropy - i)

    max_index = np.argmax(D)  # 最大值的下标
    data_branch = dict_items[x_labels[max_index]]
    tree = {x_labels[max_index]: {}}
    for s in data_branch:
        next_indices = np.where(data[:, max_index] == s)[0]  # 生成式多维的
        next_data = data[next_indices, :]
        next_data = np.delete(next_data, max_index, axis=1)
        next_x_labels = np.delete(x_labels, max_index)
        tree[x_labels[max_index]][s] = cal_entropy(next_data, next_x_labels)
    return tree

# 绘制树状图
# 创建图
dot = Digraph()

# 定义决策树结构
node_counter = 0

def add_nodes_edges(tree, parent_name):
    global node_counter  # 引用全局变量

    for key, value in tree.items():
        print(key, value)

        if isinstance(value, dict):  # 如果是字典，说明还有子树
            node_counter += 1
            node_name = f"{parent_name}_{node_counter}"  # 生成唯一节点名称
            dot.node(node_name, key, fontname='SimHei')  # 添加节点
            dot.edge(parent_name, node_name)  # 连接边
            add_nodes_edges(value, node_name)  # 递归
        else:  # 否则是叶子节点
            node_counter += 1
            leaf_node_name = f"{parent_name}_{node_counter}"  # 生成唯一叶子节点名称
            dot.node(leaf_node_name, key, fontname='SimHei')  # 添加节点
            dot.edge(parent_name, leaf_node_name)  # 连接边

            node_counter += 1
            value_node_name = f"{leaf_node_name}_{node_counter}"  # 生成唯一叶子节点名称
            dot.node(value_node_name, value, fontname='SimHei')  # 添加叶子节点
            dot.edge(leaf_node_name, value_node_name)  # 连接边

def predict(tree, x_labels, sample):
    if not isinstance(tree, dict):
        return tree
    root = next(iter(tree))
    subtree = tree[root]
    feature_index = x_labels.index(root)
    feature_value = sample[feature_index]
    if feature_value in subtree:
        return predict(subtree[feature_value], x_labels, sample)
    else:
        return None

if __name__ == "__main__":
    data2 = ["青绿", "乌黑", "乌黑", "青绿", "浅白", "青绿", "乌黑", "乌黑",
             "乌黑", "青绿", "浅白", "浅白", "青绿", "浅白", "乌黑", "浅白", "青绿"]
    data6 = ["蜷缩", "蜷缩", "蜷缩", "蜷缩", "蜷缩", "稍蜷", "稍蜷", "稍蜷",
             "稍蜷", "硬挺", "硬挺", "蜷缩", "稍蜷", "稍蜷", "稍蜷", "蜷缩", "蜷缩"]
    data3 = ["浊响", "沉闷", "浊响", "沉闷", "浊响", "浊响", "浊响", "浊响",
             "沉闷", "清脆", "清脆", "浊响", "浊响", "沉闷", "浊响", "浊响", "沉闷"]
    data4 = ["清晰", "清晰", "清晰", "清晰", "清晰", "清晰", "稍糊", "清晰",
             "稍糊", "清晰", "模糊", "模糊", "稍糊", "稍糊", "清晰", "模糊", "稍糊"]
    data5 = ["凹陷", "凹陷", "凹陷", "凹陷", "凹陷", "稍凹", "稍凹", "稍凹",
             "稍凹", "平坦", "平坦", "平坦", "凹陷", "凹陷", "稍凹", "平坦", "稍凹"]
    data1 = ["硬滑", "硬滑", "硬滑", "硬滑", "硬滑", "软粘", "软粘", "硬滑",
             "硬滑", "软粘", "硬滑", "软粘", "硬滑", "硬滑", "软粘", "硬滑", "硬滑"]
    result = ["ok", "ok", "ok", "ok", "ok", "ok", "ok", "ok",
              "ng", "ng", "ng", "ng", "ng", "ng", "ng", "ng", "ng"]
    x_labels = ["触感", "色泽", "敲声", "纹理", "脐部", "根蒂"]
    data = np.array([data1, data2, data3, data4, data5, data6, result]).transpose()  # 矩阵进行转置,得到data
    # 调用cal_entropy函数
    tree = cal_entropy(data, x_labels)
    print(tree)
    # 可视化决策树
    # 画决策树
    dot.attr(fontname='SimHei')
    # 画决策树
    add_nodes_edges(tree, 'root')  # 假设 'root' 是根节点的名称

    # 渲染图像
    dot.render('decision_tree', format='png', view=True)

    # 预测示例
    sample = ["硬滑", "青绿", "浊响", "清晰", "凹陷", "蜷缩"]
    prediction = predict(tree, x_labels, sample)
    print(f"Prediction for sample {sample}: {prediction}")
```



```
import numpy as np
import pandas as pd

# 定义属性及其对应的可能取值
attributes = {
    '触感': np.array(['硬滑', '软粘']),
    '色泽': np.array(['乌黑', '浅白', '青绿']),
    '敲声': np.array(['沉闷', '浊响', '清脆']),
    '纹理': np.array(['模糊', '清晰', '稍糊']),
    '脐部': np.array(['凹陷', '平坦', '稍凹']),
    '根蒂': np.array(['硬挺', '稍蜷', '蜷缩']),
}

# 生成随机样本数量
num_samples = 100  # 可以根据需要调整样本数量

# 生成数据集
data = np.array([np.random.choice(values, size=num_samples) for values in attributes.values()]).T


# 显示生成的随机数据集
print(data)
```



```
# -*- coding: utf-8 -*-
import numpy as np
import pydotplus
from graphviz import Digraph

def cal_entropy(data, x_labels):
    finish_columns = np.unique(data[:, -1])
    if len(finish_columns) == 1:
        return finish_columns[0]  # 返回ng与ok，表面这个分支已经分到叶子了
    dict_items = dict()  # 创建一个字典,储存有哪些类别
    len_num, len_label = data.shape  # 总类别数, 每一组总的个数
    len_label -= 1
    result = data[:, len_label]
    unique_decision = np.unique(result)  # ok, ng
    len_decision_ok = np.sum(result == unique_decision[0])  # ok的个数
    len_decision_nor = len(result) - len_decision_ok
    old_entropy = -float(len_decision_ok / len_num) * np.log2(float(len_decision_ok / len_num)) - float(len_decision_nor / len_num) * np.log2(float(len_decision_nor / len_num))

    message_entropy = []
    for i in range(len_label):
        item = data[:, i]
        unique_item = np.unique(item)  # 第i种类别的有哪些
        dict_items[x_labels[i]] = unique_item
        len_item = len(unique_item)  # 第i种类别的有多少种
        entropy_item = []
        entropy_item_len = []

        for j in range(len_item):
            # 遍历item，找到unique_item中每一个类别的下标
            indexs = np.where(item == unique_item[j])
            len_indexs = len(indexs[0])  # where返回的是一个元组，第一个元素是下标
            entropy_item_len.append(len_indexs)

            # 遍历result，找到对应的值
            result_item = result[indexs[0]]  # 使用 indexs[0]
            len_ok = np.sum(result_item == unique_decision[0])
            len_nor = len_indexs - len_ok

            # 计算信息熵
            if len_indexs > 0:  # 确保 len_indexs 不为零
                p1 = float(len_ok) / len_indexs
                p2 = float(len_nor) / len_indexs

                # 避免计算 log(0)
                entropy_value = 0
                if p1 > 0:
                    entropy_value -= p1 * np.log2(p1)
                if p2 > 0:
                    entropy_value -= p2 * np.log2(p2)

                entropy_item.append(entropy_value)
            else:
                entropy_item.append(0)

        # 计算加权熵
        entropy = 0
        for j in range(len_item):
            p = float(entropy_item_len[j]) / float(len_num)
            entropy += p * entropy_item[j]
        message_entropy.append(entropy)
    D = []
    for i in message_entropy:
        D.append(old_entropy - i)

    max_index = np.argmax(D)  # 最大值的下标
    data_branch = dict_items[x_labels[max_index]]
    tree = {x_labels[max_index]: {}}
    for s in data_branch:
        next_indices = np.where(data[:, max_index] == s)[0]  # 生成式多维的
        next_data = data[next_indices, :]
        next_data = np.delete(next_data, max_index, axis=1)
        next_x_labels = np.delete(x_labels, max_index)
        tree[x_labels[max_index]][s] = cal_entropy(next_data, next_x_labels)
    return tree

def visualize_tree(tree, dot=None):
    if dot is None:
        dot = Digraph()
        dot.attr(fontname="SimHei")  # 设置字体为SimHei
    for key, value in tree.items():
        if isinstance(value, dict):
            for sub_key, sub_value in value.items():
                dot.node(sub_key, sub_key, fontname="SimHei")
                dot.edge(key, sub_key, label=sub_key, fontname="SimHei")
                visualize_tree({sub_key: sub_value}, dot)
        else:
            dot.node(value, value, fontname="SimHei")
            dot.edge(key, value, label=str(value), fontname="SimHei")
    return dot

def predict(tree, x_labels, sample):
    if not isinstance(tree, dict):
        return tree
    root = next(iter(tree))
    subtree = tree[root]
    feature_index = x_labels.index(root)
    feature_value = sample[feature_index]
    if feature_value in subtree:
        return predict(subtree[feature_value], x_labels, sample)
    else:
        return None

if __name__ == "__main__":
    data2 = ["青绿", "乌黑", "乌黑", "青绿", "浅白", "青绿", "乌黑", "乌黑",
             "乌黑", "青绿", "浅白", "浅白", "青绿", "浅白", "乌黑", "浅白", "青绿"]
    data6 = ["蜷缩", "蜷缩", "蜷缩", "蜷缩", "蜷缩", "稍蜷", "稍蜷", "稍蜷",
             "稍蜷", "硬挺", "硬挺", "蜷缩", "稍蜷", "稍蜷", "稍蜷", "蜷缩", "蜷缩"]
    data3 = ["浊响", "沉闷", "浊响", "沉闷", "浊响", "浊响", "浊响", "浊响",
             "沉闷", "清脆", "清脆", "浊响", "浊响", "沉闷", "浊响", "浊响", "沉闷"]
    data4 = ["清晰", "清晰", "清晰", "清晰", "清晰", "清晰", "稍糊", "清晰",
             "稍糊", "清晰", "模糊", "模糊", "稍糊", "稍糊", "清晰", "模糊", "稍糊"]
    data5 = ["凹陷", "凹陷", "凹陷", "凹陷", "凹陷", "稍凹", "稍凹", "稍凹",
             "稍凹", "平坦", "平坦", "平坦", "凹陷", "凹陷", "稍凹", "平坦", "稍凹"]
    data1 = ["硬滑", "硬滑", "硬滑", "硬滑", "硬滑", "软粘", "软粘", "硬滑",
             "硬滑", "软粘", "硬滑", "软粘", "硬滑", "硬滑", "软粘", "硬滑", "硬滑"]
    result = ["ok", "ok", "ok", "ok", "ok", "ok", "ok", "ok",
              "ng", "ng", "ng", "ng", "ng", "ng", "ng", "ng", "ng"]
    x_labels = ["触感", "色泽", "敲声", "纹理", "脐部", "根蒂"]
    data = np.array([data1, data2, data3, data4, data5, data6, result]).transpose()  # 矩阵进行转置,得到data
    print(data)
    # 调用cal_entropy函数
    tree = cal_entropy(data, x_labels)
    print(tree)
    # 可视化决策树
    dot = visualize_tree(tree)
    dot.render('decision_tree', format='png', view=True)
    # 预测示例
    sample = ["硬滑", "青绿", "浊响", "清晰", "凹陷", "蜷缩"]
    prediction = predict(tree, x_labels, sample)
    print(f"Prediction for sample {sample}: {prediction}")
```









```
# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split

def cal_entropy(data, x_labels):
    finish_columns = np.unique(data[:, -1])
    if len(finish_columns) == 1:
        return finish_columns[0]  # 返回ng与ok，表面这个分支已经分到叶子了
    dict_items = dict()  # 创建一个字典,储存有哪些类别
    len_num, len_label = data.shape  # 总类别数, 每一组总的个数
    len_label -= 1
    result = data[:, len_label]
    unique_decision = np.unique(result)  # ok, ng
    len_decision_ok = np.sum(result == unique_decision[0])  # ok的个数
    len_decision_nor = len(result) - len_decision_ok
    old_entropy = -float(len_decision_ok / len_num) * np.log2(float(len_decision_ok / len_num)) - float(len_decision_nor / len_num) * np.log2(float(len_decision_nor / len_num))

    message_entropy = []
    for i in range(len_label):
        item = data[:, i]
        unique_item = np.unique(item)  # 第i种类别的有哪些
        dict_items[x_labels[i]] = unique_item
        len_item = len(unique_item)  # 第i种类别的有多少种
        entropy_item = []
        entropy_item_len = []

        for j in range(len_item):
            # 遍历item，找到unique_item中每一个类别的下标
            indexs = np.where(item == unique_item[j])
            len_indexs = len(indexs[0])  # where返回的是一个元组，第一个元素是下标
            entropy_item_len.append(len_indexs)

            # 遍历result，找到对应的值
            result_item = result[indexs[0]]  # 使用 indexs[0]
            len_ok = np.sum(result_item == unique_decision[0])
            len_nor = len_indexs - len_ok

            # 计算信息熵
            if len_indexs > 0:  # 确保 len_indexs 不为零
                p1 = float(len_ok) / len_indexs
                p2 = float(len_nor) / len_indexs

                # 避免计算 log(0)
                entropy_value = 0
                if p1 > 0:
                    entropy_value -= p1 * np.log2(p1)
                if p2 > 0:
                    entropy_value -= p2 * np.log2(p2)

                entropy_item.append(entropy_value)
            else:
                entropy_item.append(0)

        # 计算加权熵
        entropy = 0
        for j in range(len_item):
            p = float(entropy_item_len[j]) / float(len_num)
            entropy += p * entropy_item[j]
        message_entropy.append(entropy)
    D = []
    for i in message_entropy:
        D.append(old_entropy - i)

    max_index = np.argmax(D)  # 最大值的下标
    data_branch = dict_items[x_labels[max_index]]
    tree = {x_labels[max_index]: {}}
    for s in data_branch:
        next_indices = np.where(data[:, max_index] == s)[0]  # 生成式多维的
        next_data = data[next_indices, :]
        next_data = np.delete(next_data, max_index, axis=1)
        next_x_labels = np.delete(x_labels, max_index)
        tree[x_labels[max_index]][s] = cal_entropy(next_data, next_x_labels)
    return tree

def predict(tree, x_labels, sample):
    if not isinstance(tree, dict):
        return tree
    root = next(iter(tree))
    subtree = tree[root]
    feature_index = x_labels.index(root)
    feature_value = sample[feature_index]
    if feature_value in subtree:
        return predict(subtree[feature_value], x_labels, sample)
    else:
        return None

def score(tree, x_labels, X_test, y_test):
    correct = 0
    for sample, true_label in zip(X_test, y_test):
        prediction = predict(tree, x_labels, sample)
        if prediction == true_label:
            correct += 1
    return correct / len(y_test)

if __name__ == "__main__":
    # 加载数据集
    wine = load_wine()
    feature_names = wine.feature_names
    target_names = wine.target_names

    # 将数据集转换为DataFrame并合并
    wine_data = pd.concat([pd.DataFrame(wine.data, columns=feature_names), pd.DataFrame(wine.target, columns=['target'])], axis=1)

    # 分割数据集为训练集和测试集
    Xtrain, Xtest, Ytrain, Ytest = train_test_split(wine.data, wine.target, test_size=0.3, random_state=42)

    # 将训练集和测试集合并为一个数组
    train_data = np.column_stack((Xtrain, Ytrain))
    test_data = np.column_stack((Xtest, Ytest))

    # 调用cal_entropy函数训练决策树
    tree = cal_entropy(train_data, feature_names)
    print(tree)

    # 预测示例
    sample = Xtest[0]
    prediction = predict(tree, feature_names, sample)
    prediction_index = int(prediction)  # 获取预测结果的索引
    print(f"Prediction for sample {sample}: {target_names[prediction_index]}")

    # 计算模型在测试集上的准确率
    accuracy = score(tree, feature_names, Xtest, Ytest)
    print(f"Model accuracy: {accuracy}")
```